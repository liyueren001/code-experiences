{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Joshua-Woodard/Text_Generation_RNN_GRU/blob/main/Text_Generation_RNN_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMYiFhRSF3ZT"
   },
   "source": [
    "## Text Generation using RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRn7NGrECFXR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBeRYCMqCIfY",
    "outputId": "4cd1a9fe-3101-4d5c-9d0a-993db7a9a9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n",
      "1130496/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download dataset (Shakespeare)\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XT9Wcg9mCSuj",
    "outputId": "7f337cc3-8618-4d90-9985-1d4f5f15a459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compatibility\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ekuCsolCh7G",
    "outputId": "a43a5973-c171-4eb4-fa84-1d082af7f94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine data\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGAx0V1jCmTq",
    "outputId": "80e05571-c325-4dd5-a4f9-5021c2f435b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# How many unique characters in file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3KMnBtVF8ld"
   },
   "source": [
    "### Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQ2OMqmOCt3w"
   },
   "outputs": [],
   "source": [
    "# Vectorize Text\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), \n",
    "    mask_token=None\n",
    ")\n",
    "\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
    ")\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.string.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3lLJoX0Dn-Q",
    "outputId": "761ae9a0-814a-4b2e-ebc8-47099dcf8657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFqWGrsAEnwt"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRI-JgalEsOV",
    "outputId": "edd4bbe3-a5de-40e7-b420-a0b40944a7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqhVTqIIEyN4"
   },
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEZcc2ukE0pO",
    "outputId": "218824c1-d6a0-4f34-9844-428dc28e6e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ3434pLE8hH"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "  input_text = sequence[:-1]\n",
    "  target_text = sequence[1:]\n",
    "  return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tUalfRQFPvR",
    "outputId": "febde28a-80c5-4221-bd6b-ac4e3b8e3a33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPgMlX03FSiF"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnIKsKi3F0q2"
   },
   "source": [
    "### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jc5CUdxWFWs9",
    "outputId": "064cab39-5e21-4e4a-cd70-cacb2b53ea8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsiTpTHHGCGY"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik1P6gLPFePE"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# Embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGPCn85OGSHL"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SW3PPXC9H6XS"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOC2w4Q6IA4Z"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yjNnWeSIckw"
   },
   "outputs": [],
   "source": [
    "# configure checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dezPSNs3JPN6",
    "outputId": "8a709c32-d19b-4f31-dbf3-b227617ee78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 13s 43ms/step - loss: 2.7057 - accuracy: 0.2794\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.9812 - accuracy: 0.4196\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 1.6983 - accuracy: 0.4958\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 1.5390 - accuracy: 0.5375\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.4428 - accuracy: 0.5625\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.3758 - accuracy: 0.5792\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.3245 - accuracy: 0.5920\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.2798 - accuracy: 0.6032\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.2391 - accuracy: 0.6138\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.1999 - accuracy: 0.6237\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.1593 - accuracy: 0.6357\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 1.1194 - accuracy: 0.6463\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 1.0754 - accuracy: 0.6594\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 9s 41ms/step - loss: 1.0290 - accuracy: 0.6734\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 0.9809 - accuracy: 0.6880\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 0.9301 - accuracy: 0.7044\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.8775 - accuracy: 0.7212\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 0.8261 - accuracy: 0.7375\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 0.7747 - accuracy: 0.7543\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.7273 - accuracy: 0.7696\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.6822 - accuracy: 0.7842\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 0.6454 - accuracy: 0.7956\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.6113 - accuracy: 0.8062\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 8s 41ms/step - loss: 0.5812 - accuracy: 0.8157\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.5556 - accuracy: 0.8234\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.5353 - accuracy: 0.8295\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.5163 - accuracy: 0.8349\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.5012 - accuracy: 0.8389\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.4878 - accuracy: 0.8428\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 8s 40ms/step - loss: 0.4766 - accuracy: 0.8457\n"
     ]
    }
   ],
   "source": [
    "history_txtgen = model.fit(dataset,\n",
    "                           epochs=30,\n",
    "                           callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "forXvmjhKg5b"
   },
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpXl7cezJVxn"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zHEG8S7KgQl"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model,chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZSKwpveKnK8",
    "outputId": "81f7c793-4992-4f0a-a063-9975498eaf5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOSHUA:\n",
      "I swear, my lord, where shame doth call me.\n",
      "\n",
      "CLAUDIO:\n",
      "Thus might you have, better do another duke.\n",
      "Dost thou behold the oath debt with the\n",
      "teeth and the duke is was but newd, and baskly, and\n",
      "then the red of the world's hand, and with his good as withstard\n",
      "By and by colour'd in his ere I'll acquit myself:\n",
      "A creature is possessed with all.\n",
      "\n",
      "SICINIUS:\n",
      "To except my horse,\n",
      "I mean, his a mortal fortune of my fortune!\n",
      "Hath she use med son.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "O, shadows than ever thou, unsweat hath!\n",
      "Where is my Roman what is thy house, 'tis round;\n",
      "Happy wife and mine and low shall\n",
      "The Lord Hastings, then beloved is hand.\n",
      "\n",
      "BAPTISTA:\n",
      "I might take her to prison! lay both they\n",
      "are choice: then we shall ever be gone.\n",
      "Within this noble heart love notice,\n",
      "He must be, this hand and many lips,\n",
      "Of breach ower to again the rock.\n",
      "\n",
      "CORIOLANUS:\n",
      "I mude away,\n",
      "If not my deceive any fair discreatures be\n",
      "full-poping true without all, and much lengt\n",
      "My succession with most pawn'd him hides\n",
      "His regignce is meeping \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run Time: 2.722952127456665\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['JOSHUA:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end=time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_a-k6kboLOnI",
    "outputId": "89bdf6e7-8489-4daf-c109-6d1a9775cbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"JOSHUA:more! what's thy name?\\n\\nBENVOLIO:\\nRomeo! my cousin!--\\nAll his friends whither away. But he hath something me\\nAnd say 'What is't your books: which, vilau-dea,\\nand that the sun that love in sight,\\nAnd now comfost and in company,\\nNo imposting eyes for thee, nor any man?\\nShall I not hear it? I mean, he hasks it:\\nIf the instance we demand, and to be obstered\\nin the time of Gloucester and the key,\\nThat longing hours hide more voices of men:\\nhis rascul arms was in rut of heart.\\nKing Clifford and Bolingbroke!\\n\\nKING RICHARD III:\\nCall it not present to inquire means\\nTo pluck the window-blow thou art.\\n\\nGREMIO:\\nWhy should you bide? O that I were as for a world;\\nI'll bring what is the whole language and tear-long dut\\nLord Angelo is vow a subtle out.\\n\\nDUCHESS OF YORK:\\nO, let me live in thy raise, what have done\\nIt shall be parch'd thee to the crown.\\n\\nKING RICHARD II:\\nHere would inclinent to my body to turn\\nThe base issue of the death of kings.\\nMy father took a king of late to blame Means awake\\nAs fo\"\n",
      " b\"ADAM:an't please you to go and smile.\\nI beseech thee, for this night, let beauteous present,\\nWith all ready for thee here in Gaunt\\nAnd banish with a tear of thee, daughter, wipenifuld,\\nNor the most musician of suspocious stuff,\\nA courage makes your apence, and I cannot be.\\n\\nBRUTUS:\\nAn o'er what I could have sent\\nmeaty thank my present hand,\\nAnd start and care he met their aid within this hustable.\\n\\nHENRY BOLINGBROKE:\\nSirrah, let me be thou art to hear\\nI to deny me of this: Looking with a lightning lambs;\\nHarbon there; be it more veil'd.\\n\\nCLARENCE:\\n\\nKING EDWARD IV:\\nWelcome, be thou a prefent wood,\\nScaling join ourselves, ourselves our sword,\\nAnd wrinkle a wild defend time of day and heir\\nAwaked your pleasure hunger, shall scare make to a disclace,\\nAnd after that suitors from all the fashions,\\nIn help'd than from our sceptre such very house.\\n\\nMERCUTIO:\\nNay, let them go; and withal his maid\\nfor perjury, in the cham.\\n\\nGEBOLY:\\nCome, being true, i' the loss of heaven.\\nBefore his maid! how farewel\"\n",
      " b\"BRANDON:\\n'I'll make a woman and we fear'd and labour\\nBy undersal. Beware you the cap\\nAbout it comes on: go to;\\nYou are a torch for mouths, whereon you warrant,\\nSay, Somerpit his court'sy hand and self.\\nO: now, I do; she may be with you now!\\n\\nKING EDWARD IV:\\nWhy cheque me too? how let it strike thee?\\nHave thrice disturbs of the duke.\\n\\nDUKE VINCENTIO:\\nKing Herena, make it thee with inherit,\\nBut man found me with this fool deliver,\\nThis noble honourable maid that extreme\\nsight, trust me, 'tis time but grieved all together,\\nNot so but ever her for ever.\\nThis pretty eaglem will be offended none,\\nIf not but weeps and eafs and repty high\\nAs eating thee for ever.'\\n\\nADRIAN:\\nTuture of your avenmiors,\\nIf that by rewer in Bohemia, from her love a woman's hateful.\\n\\nWERWICK:\\nThen let my talk upon Himand. What can he lives to year?\\nHave I in not able, for so grieved as I,\\nOr lace first bun abroad, 'tis to report\\nAt you. Lad as her alone, O, sir;\\nI will not frowning, farewell. Humbly wife,\\nTo sive an honest I\"\n",
      " b\"RICHARD:\\nThen, by the king your father's love, thy brother's wife\\nWith heighby tidings of his dee.\\n\\nSICINIUS:\\nWhere should this true?\\n\\nJULIET:\\nGood time, or else a friend perjured Henry's house,\\nTo prove him, in heaven bend thy helm to his beasts,\\nI'll play thee work! here is thy child.\\n\\nCLAUDIO:\\nFly to us All:\\nThat's he must know, when my fellow or youth,\\nAnd in King Richard how it please you to your charge,\\nUnless to it ordert; I will bear my face.\\nI am ashealth. He call'd me king,\\nAnd in my stirr'd applary and tells,\\nUnless the easy danger out of Marcius' tongue\\nAgainst my count,\\nNot light them fairly richest pelfer it.\\nIn that I could for thee to thy love and marriage?\\n\\nPAULINA:\\nEven here thou tyran?\\nWord you will prove him, and not deny to die?\\n3 KING HENRY VI\\n\\nRIVERS:\\nMore than I were: then 'tis three his redignt.\\n\\nEXETER:\\nHis gow to this mad congemnation!\\n\\nDUCHESS OF YORK:\\nNay, but he's content to die than see\\nOur prisoners of his tongue.\\n\\nANTIGONUS:\\nNow ke still have fronked them succe\"], shape=(4,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.560478448867798\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['JOSHUA:', 'ADAM:', 'BRANDON:', 'RICHARD:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ny9a2nIOZJq"
   },
   "source": [
    "### Save Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdIn7A6rOtIf",
    "outputId": "ea63c4bb-b5c6-44c3-dc35-fd32a022c31a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f3f06778590>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f3f06778590>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_9_layer_call_fn, gru_cell_9_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijCOd3g-Ov3u"
   },
   "source": [
    "#### NOTE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rojFIORrXsW7"
   },
   "source": [
    "If need more in-depth/customized training for text generator, see https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbOhlYBTXpnD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN6W94G1BOFXwnchKAkoGba",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Text_Generation_RNN_GRU.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
